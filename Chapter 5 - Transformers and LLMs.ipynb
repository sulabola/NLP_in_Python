{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5371c45-5a3c-48f7-83d1-a11f1001216f",
   "metadata": {},
   "source": [
    "### Transformers and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17158e38-72b5-4234-9ad2-7c5f4a9d1a54",
   "metadata": {},
   "source": [
    "Transformers are the architecture behind ChatGPT and many other powerful LLMs (Large Language models).\n",
    "\n",
    "LLMs are specifically designed to perform NLP tasks.\n",
    "\n",
    "In this chapter, we will discuss the details of Transformers and LLMs. Then we dive into three main layers of transformers: embedding layers, attention layers, and an FNN layer. Next, we will be discussing categories of transformers: encoder, decoder, and encoder & decoder models. Last, we discuss details of popular LLMs: GPT, BERT,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f972a4c-d21f-4078-8ac3-73df02065861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec0b5bd-d6c2-4c89-bd18-a7fe9ffb5d61",
   "metadata": {},
   "source": [
    "Transformers are a deep learning architecture. That is combining deep learning with other calculations and passing data through a series of steps to extract meaningful patterns and make predictions. It consists of three main layers: embeddings, attention, and FNN\n",
    "\n",
    "Also, we have access to LLMs. These are the deep learning models that have been pretrained on massive amounts of text data.\n",
    "\n",
    "In some cases, they overlap. We have transformer-based LLMs. These are popular deep learning approaches to NLP tasks. Note that Transformers can also be used for other purposes: cision, audio,... On the other hand, LLMs are there that are based on RNN, LSTM,... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8bbe5-a5e1-4228-bedd-a2590b28f9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b44614-8c4a-4daa-8da7-cc4f311214a3",
   "metadata": {},
   "source": [
    "**Transformer Architecture**\n",
    "\n",
    "Along the way, the input text gradually transformed, hence they are called transformers.\n",
    "\n",
    "Main Layers:\n",
    "\n",
    "Raw text -> embeddings layer -> Attention layer -> FNN layer -> Prediction\n",
    "\n",
    "- Embeddings layer: Use vectors to represent the semantic meaning of words. This is similar to vectorization. However, these embeddings are very smart vectors. Because each vector points to a space, and it has a meaning.\n",
    "- Attention layer: Make adjustments to vectors/embeddings. This adjusts the vector slightly based on the context of surrounding words. (make the word more meaningful).\n",
    "- FNN layer: learn new features, identify patters and include it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac6cf9-f728-41f4-9c78-6e1a5fa1fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a63c516-5555-408b-860c-b2757b503c26",
   "metadata": {},
   "source": [
    "**Embeddings**\n",
    "\n",
    "Here we are converting text tokens into meaningful numeric representations.\n",
    "\n",
    "Each token (word) is placed into a high-dimensional space. Thus, words with similar meanings end up close together.\n",
    "\n",
    "In general, each word is represented with a large number of dimensions (typically 768). The value on each dimension is not random. The values have semantic meaning. These values are generated based on popular word embeddings trained using shallow neural networks (word2vec) and matrix factorization (GloVe). (shallow neural network -  neural network with a single hidden layer). Within LLMs, the values are randomly initialized and slowly updated until they reach their final values (as in neural networks). The parameters in neural networks are represented as matrices, and these embedding matrices are exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c97b15-fc71-4a61-80a5-b380cd165bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d018d7d3-b4cf-4964-8581-a8efa18e0adc",
   "metadata": {},
   "source": [
    "**Attention**\n",
    "\n",
    "This is the most important component in transformer architecture. Here, it adds context by helping each token absorb additional meaning from other tokens.\n",
    "\n",
    "Example: \"lemonade\", \"I love cold lemonade\", \"I love Beyonce's Lemonade album\"\n",
    "\n",
    "Let's consider the phrase \"I love cold lemonade.\" The token here is \"lomonade.\" With the attention layer, the word \"cold\" adds context to lemonade. In technical terms, cold attends to lemonade. In simpler words, it is not just a lemonade; it is a cold lemonade. Similarly, love attends to lemonade.\n",
    "In the attention layer, it does this by creating matrices for queries, keys, and attention scores. Similar to embeddings layer, the query and key values are randomly initiated and slowly updated until they reach their final values. To capture query-key similarity, a dot product (similarity score) is taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea9033-9e4b-455e-beb1-755dcd7095a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e8393d-ae1b-47ef-a570-a7d318062229",
   "metadata": {},
   "source": [
    "**Feedforward Neural Network**\n",
    "\n",
    "In this layer, patterns in those contextual relationships are learned and captured.\n",
    "\n",
    "Typical FNN in a transformer:\n",
    "\n",
    "Input layer -> Hidden layer -> Output layer\n",
    "\n",
    "Input is what's coming from the attention layer, and output is what we send to the prediction. Typically, we have 768 nodes in inout layer, 3072 nodes in the hidden layer, and 768 nodes in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753a250-cee2-4483-8891-5a5c6fb12304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "059069dc-2679-4c1e-99a2-2849b33b6494",
   "metadata": {},
   "source": [
    "How do transformers work so well?\n",
    "\n",
    "It is due to the attention layer. It adds context (enriches the meaning of each word based on others).\n",
    "\n",
    "Due to the structure of the attention layer (matrices), it allows for parallelization. Thus, it can be trained fast.\n",
    "\n",
    "Attention generalizes well (core transformer architecture works well for a variety of NLP tasks).\n",
    "\n",
    "In reality, the layers typically follow this order:\n",
    "\n",
    "Raw text -> Embeddings layer -> Transformer block 1 (Attention layer + FNN) -> Transformer block 2 -> ... -> Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a88c0c-b764-4f9f-a203-8ea841415d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df5454cf-c3f9-4156-ac2c-c519a47accba",
   "metadata": {},
   "source": [
    "**Categories of Transformers**\n",
    "\n",
    "Mainly, there are three categories of transformers: encoder-only models, decoder-only models, and encoder-decoder models.\n",
    "\n",
    "Different models will use different pieces of the transformer architecture.\n",
    "\n",
    "- Encoder-Only Models: The encoder takes raw text and encodes it as an embedding representation of the text (transforms text into a vector). In short, it understands text. Thus, the application here is sentiment analysis (Output: positive/negative). In summary, transform text into a vector, and the vector has a meaning.\n",
    "\n",
    "- Decoder-Only Models: The decoder takes an input text sequence and infers (predicts) the next word. This will give a probability distribution for the next word. In short, it generates text.\n",
    "\n",
    "- Encoder-Decoder Models: These modes takes two inputs: a text sequence and a shifted target sequence. The target sequence is shifted because we want to predict the next word. Both inputs are encoded as embeddings and combined to infer the next word. In short, it understands and generates text. Thus, translation would be an application of this.\n",
    "\n",
    "How do we use these in practice? We would download a pretrained LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f37cfd-6dbe-4e37-a8de-d90b225bc186",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5fcc63-f20d-43bb-bd1c-91c408cd7b87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
