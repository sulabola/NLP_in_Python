{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5371c45-5a3c-48f7-83d1-a11f1001216f",
   "metadata": {},
   "source": [
    "### Transformers and LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17158e38-72b5-4234-9ad2-7c5f4a9d1a54",
   "metadata": {},
   "source": [
    "Transformers are the architecture behind ChatGPT and many other powerful LLMs (Large Language models).\n",
    "\n",
    "LLMs are specifically designed to perform NLP tasks.\n",
    "\n",
    "In this chapter, we will discuss the details of Transformers and LLMs. Then we dive into three main layers of transformers: embedding layers, attention layers, and an FNN layer. Next, we will be discussing categories of transformers: encoder, decoder, and encoder & decoder models. Last, we discuss details of popular LLMs: GPT, BERT,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f972a4c-d21f-4078-8ac3-73df02065861",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eec0b5bd-d6c2-4c89-bd18-a7fe9ffb5d61",
   "metadata": {},
   "source": [
    "Transformers are a deep learning architecture. That is combining deep learning with other calculations and passing data through a series of steps to extract meaningful patterns and make predictions. It consists of three main layers: embeddings, attention, and FNN\n",
    "\n",
    "Also, we have access to LLMs. These are the deep learning models that have been pretrained on massive amounts of text data.\n",
    "\n",
    "In some cases, they overlap. We have transformer-based LLMs. These are popular deep learning approaches to NLP tasks. Note that Transformers can also be used for other purposes: cision, audio,... On the other hand, LLMs are there that are based on RNN, LSTM,... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8bbe5-a5e1-4228-bedd-a2590b28f9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22b44614-8c4a-4daa-8da7-cc4f311214a3",
   "metadata": {},
   "source": [
    "Transformer Architecture\n",
    "\n",
    "Along the way, the input text gradually transformed, hence they are called transformers.\n",
    "\n",
    "Main Layers:\n",
    "\n",
    "Raw text -> embeddings layer -> Attention layer -> FNN layer -> Prediction\n",
    "\n",
    "- Embeddings layer: Use vectors to represent the semantic meaning of words. This is similar to vectorization. However, these embeddings are very smart vectors. Because each vector points to a space, and it has a meaning.\n",
    "- Attention layer: Make adjustments to vectors/embeddings. This adjusts the vector slightly based on the context of surrounding words. (make the word more meaningful).\n",
    "- FNN layer: learn new features, identify patters and include it into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ac6cf9-f728-41f4-9c78-6e1a5fa1fe6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63c516-5555-408b-860c-b2757b503c26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
